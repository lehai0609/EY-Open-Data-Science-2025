# Project Status Report: Urban Heat Island (UHI) Prediction as of 14-Feb-2025

## 1. Problem Statement

**Objective:**

The primary goal of this project is to develop an open-source machine learning model that accurately predicts the Urban Heat Island (UHI) Index at a micro-scale (meter-level resolution) across urban areas in New York City. The model should also identify the key factors contributing to UHI hotspots, providing actionable insights for urban planners and decision-makers to mitigate the UHI effect.

**Input Datasets:**

The following datasets are currently being used as model inputs:

*   **Ground-Based Air Temperature Data:**
    *   Collected on July 24, 2021, between 3:00 PM and 4:00 PM in Manhattan and the Bronx.
    *   Includes UHI Index values (target variable), calculated as the ratio of a location’s temperature to the city’s average temperature.
*   **Building Footprint Data:**
    *   Provides information on building locations, density, and configurations (geographic building footprints, building height).
*   **Satellite Data:**
    *   **Sentinel-2 (European Space Agency):** High-resolution optical data, including vegetation indices like NDVI.
    *   **Landsat (NASA):** Thermal infrared data capturing land surface temperatures (LST).
*   **Local Weather Data:**
    *   Includes temperature, humidity, wind speed and direction, and solar flux.
    *   Collected at two weather stations within the data region, providing near-surface measurements every 5 minutes.

*   **Additional Publicly Available Datasets (Optional):**
    *   Socioeconomic data (e.g., population density, income levels).
    *   Urban activity (e.g., traffic density).
    *   Pollution and air quality indices.
    *   Energy Usage datasets
    *   Vegetation and green space dataset such as urban tree canopy assessment
    *   Street Level Albedo & Surface Composition

**Constraints:**

*   **Computational Requirements:** The model should be runnable on a local machine with 4 cores and 32 GB of memory or a cloud-based environment.
*   **Evaluation Criteria:** Model accuracy will be primarily assessed using R-squared (coefficient of determination). Practical usability and innovation will also be considered.
*   **Data Usage:** Participants must use the provided target dataset (ground-based UHI Index).  Optional use of additional datasets is permitted but must be publicly available and properly referenced.
*   **Interpretable Models:**  Models should be simple enough to provide actionable insights but accurate enough to reflect real-world UHI dynamics.

**Output Prediction:**

The model will output a predicted UHI Index value for specific locations within the study area.  The model should also provide information on the relative importance of different features (e.g., building density, vegetation cover) in determining the UHI Index.

## 2. Determined Planning, Workflow, and Approach

The project follows a modular workflow, breaking down the problem into manageable steps and using separate Python notebooks for each step. This promotes organization, reusability, and collaboration.

**Workflow:**

1.  **Data Exploration and Preprocessing (Individual Datasets):**
    *   `01_reading_target_variables.ipynb`: Loads and preprocesses the UHI data (ground-based air temperature and UHI Index).  *Complete*. Saves processed data to `../output/uhi_data_processed.parquet`.
    *   `02_building_footprints.ipynb`: Loads and preprocesses the building footprint data, including calculating individual building areas (`calculated_area_sqm`) and reprojecting to `EPSG:2263`. *Complete*. Saves processed data to `../output/building_footprints_processed.parquet`.
    *   `03_sentinel2_data.ipynb`: Loads, preprocesses, and selects Sentinel-2 data, and calculates the NDVI. Saves processed data to `../output/sentinel2_ndvi.parquet`.
    *   `04_landsat_data.ipynb`: Loads, preprocesses, and selects Landsat data and calculates LST. Saves processed data to `../output/landsat_lst.parquet`.
    *   `05_weather_data.ipynb`: Loads and preprocesses weather data.

2.  **Feature Engineering (Individual Datasets):**
    *   `02_building_footprints.ipynb`: Calculates building density around each UHI location.  Saves updated UHI data with building density to `../output/uhi_with_building_density.parquet`.
    * `03_sentinel2_data.ipynb`: Calculate vegetation indices.
    * `04_landsat_data.ipynb`: Calculate LST related features
    *  `05_weather_data.ipynb`: Interpolate weather data to UHI locations.

3.  **Data Integration (Spatial Joins and Merging):**
    *   `06_feature_engineering.ipynb`: Loads the processed data from the Parquet files created in the previous steps. Performs spatial joins and merges all data into a single DataFrame for modeling.

4.  **Model Selection and Training:**
    *   `07_model_training.ipynb`: Trains and evaluates the machine learning model.

5.  **Model Interpretation and Actionable Insights:**
    *   `08_model_interpretation.ipynb`: Analyzes feature importance and generates insights for urban planning.

**Approach:**

*   **Modular Development:** Each step is self-contained in a separate notebook.
*   **Intermediate Data Storage:** Processed data from each step is saved to Parquet files for efficiency and reproducibility.
*   **GeoPandas for Spatial Data:**  GeoPandas is used extensively for handling geospatial data (building footprints, satellite imagery).
*   **STAC API and Planetary Computer:**  `pystac_client` and `stackstac` are used to efficiently access and process satellite imagery from the Microsoft Planetary Computer.
*   **Focus on Interpretability:**  Emphasis is placed on developing a model that is not only accurate but also provides interpretable insights into the drivers of the UHI effect.
* **Coordinate Reference System:** All data were/will be reprojected to `EPSG:2263`

**Current Status:**
I'll provide uploaded iPython notebooks along with this status update so you could take details look at the completed code.

*   `01_reading_target_variables.ipynb`
*   `02_building_footprints.ipynb`
*   `03_sentinel2_data.ipynb`
*   `04_landsat_data.ipynb`
*   `05_weather_data.ipynb`


# **Objective**
Please review the project & the content of Python notebooks then:  
- Assess its current status.  
- Check if any Python notebooks need code revisions to align with the project workflow.  
- Identify the next steps.

Here are the content of each notebooks, in markdown format

# Combined Jupyter Notebooks

## 01 Reading Target Variables

## **I. Reading target variables**

### **1. Overview of target variables**

```python
import pandas as pd

file_path = "../input/Training_data_uhi_index.csv"
df_uhi = pd.read_csv(file_path)
```

```python
print(df_uhi.head())
```

```python
df_uhi.info()
```

```python
# Convert datetime field to proper date time format
df_uhi['datetime'] = pd.to_datetime(df_uhi['datetime'], format="%d-%m-%Y %H:%M")
print(df_uhi.dtypes)
```

### **2. Descriptive Analysis**

```python
print(df_uhi.describe())

import matplotlib.pyplot as plt

# Histogram of UHI Index
plt.figure(figsize=(8, 6))
plt.hist(df_uhi['UHI Index'], bins=30, edgecolor='black')
plt.xlabel('UHI Index')
plt.ylabel('Frequency')
plt.title('Distribution of UHI Index')
plt.show()

# Scatter plot of UHI Index vs. Longitude/Latitude
plt.figure(figsize=(10, 8))
plt.scatter(df_uhi['Longitude'], df_uhi['Latitude'], c=df_uhi['UHI Index'], cmap='RdBu_r', s=10)
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title('Spatial Distribution of UHI Index')
plt.colorbar(label='UHI Index')
plt.show()
```

***Observations:***

Some thoughts from the above analysis:
- Normal distribution of UHI Index
- The spatial patterns suggest a geographical & urban distribution that influence the UHI index, as expected.

***Indication:***
- We need to incorporate spatial info into our model. Things like: building footprint, satellite, traffics.
- Good things: No missing data or outlier.

```python
import geopandas as gpd
df_uhi_geo = gpd.GeoDataFrame(df_uhi, geometry=gpd.points_from_xy(df_uhi.Longitude, df_uhi.Latitude), crs="EPSG:4326")
print(df_uhi_geo.crs)
```

## 02 Building Footprints

## **II. Building Footprint analysis**

### **1. Overview of dataset**

```python
import geopandas as gpd

file_path = "../input//Building Footprints_20250131.geojson"
gdf_buildings = gpd.read_file(file_path)
```

```python
print(gdf_buildings.head())
```

```python
print(gdf_buildings.dtypes)
```

```python
import pandas as pd
# Data Type Conversion
cols_to_convert = ['shape_area', 'heightroof', 'cnstrct_yr', 'groundelev']
for col in cols_to_convert:
    gdf_buildings[col] = pd.to_numeric(gdf_buildings[col], errors='coerce')

gdf_buildings['lstmoddate'] = pd.to_datetime(gdf_buildings['lstmoddate'], errors='coerce')
```

```python
# Descriptive Statistics and Missing Value Check
print(gdf_buildings.describe())
```

```python
# Descriptive Statistics and Missing Value Check
print(gdf_buildings.isna().sum())
```

```python
# CRS Check
print(gdf_buildings.crs)
```

### **2. Feature engineering**

```python
# 1. Reproject to EPSG:2263 (NAD83 / New York Long Island (ftUS))
gdf_buildings = gdf_buildings.to_crs(epsg=2263)

# 2. Calculate the area (now in square feet)
gdf_buildings['calculated_area_sqft'] = gdf_buildings.geometry.area

# 3. Convert square feet to square meters: 1 sq ft = 0.092903 sq m
gdf_buildings['calculated_area_sqm'] = gdf_buildings['calculated_area_sqft'] * 0.092903
```

```python
print(gdf_buildings["calculated_area_sqm"].head())
```

```python
# 4.  Outliers handling. Replace value exceeding 99.9 percentile with 99.9 percentile value
max_height = gdf_buildings['heightroof'].quantile(0.999)
gdf_buildings.loc[gdf_buildings['heightroof'] > max_height, 'heightroof'] = max_height

max_elevation = gdf_buildings['groundelev'].quantile(0.999)
gdf_buildings.loc[gdf_buildings['groundelev'] > max_elevation, 'groundelev'] = max_elevation
```

```python
print(gdf_buildings[['shape_area', 'calculated_area_sqft', 'calculated_area_sqm', 'heightroof', 'groundelev']].describe())
```

```python
print(gdf_buildings.isna().sum())
```

***Observation:***
Suggest the following approach to deal with missing data:

- Remove following fields/columns: name, base_bbl, mpluto_bbl, cnstrct_yr, doitt_id, geomsource, bin. I'd not use these fields/columns in the final analysis or model building.

- median impute for heightroof, groundelev

```python
# Columns to remove
columns_to_remove = ['name', 'base_bbl', 'mpluto_bbl', 'cnstrct_yr', 'doitt_id', 'geomsource', 'bin', 'lststatype', 'shape_len', 'globalid', 'feat_code']
gdf_buildings.drop(columns=columns_to_remove, inplace=True, errors='ignore')

# Impute missing values for 'heightroof' with the median
median_height = gdf_buildings['heightroof'].median()
gdf_buildings['heightroof'] = gdf_buildings['heightroof'].fillna(median_height)

# Impute missing values for 'groundelev' with the median
median_elevation = gdf_buildings['groundelev'].median()
gdf_buildings['groundelev'] = gdf_buildings['groundelev'].fillna(median_elevation)
```

```python
# Check missing values again
print(gdf_buildings.isna().sum())
```

```python
# Display the cleaned dataframe info
print(gdf_buildings.info())
```

```python
print(gdf_buildings.crs)
```

**Observation:**

Here are what we have achieved:
- Converted building footprint CRS to EPSG:2263.
- shape_area is corrected and in square meter(calculated_area_sqm).
- Outliers in heightroof and groundelev are handled.
- Missing values are imputed.
- Unnecessary columns are dropped.

### **3. Spatial Joining with target variables**

## 03 Sentinel-2 Data

```python
# Supress Warnings
import warnings
warnings.filterwarnings('ignore')

# Import common GIS tools
import numpy as np
import xarray as xr
import matplotlib.pyplot as plt
import rioxarray as rio
import rasterio
from matplotlib.cm import RdYlGn,jet,RdBu

# Import Planetary Computer tools
import stackstac
import pystac_client
import planetary_computer
from odc.stac import stac_load
import geopandas as gpd
import pandas as pd

# Define the bounding box for the entire data region using (Latitude, Longitude)
# This is the region of New York City that contains our temperature dataset
lower_left = (40.75, -74.01)
upper_right = (40.88, -73.86)

# Calculate the bounds for doing an archive data search
# bounds = (min_lon, min_lat, max_lon, max_lat)
bounds = (lower_left[1], lower_left[0], upper_right[1], upper_right[0])

# Define the time window
time_window = "2021-06-01/2021-09-01"

stac = pystac_client.Client.open("https://planetarycomputer.microsoft.com/api/stac/v1")

search = stac.search(
    bbox=bounds,
    datetime=time_window,
    collections=["sentinel-2-l2a"],
    query={"eo:cloud_cover": {"lt": 30}},
)

items = list(search.get_items())
print('This is the number of scenes that touch our region:',len(items))

# Define the pixel resolution for the final product
# Define the scale according to our selected crs, so we will use degrees
resolution = 10  # meters per pixel
scale = resolution / 111320.0 # degrees per pixel for crs=4326

data = stac_load(
    items,
    bands=["B01", "B02", "B03", "B04", "B05", "B06", "B07", "B08", "B8A", "B11", "B12"],
    crs="EPSG:2263", # Changed
    resolution=resolution, # Use meters directly
    chunks={"x": 2048, "y": 2048},
    dtype="uint16",
    patch_url=planetary_computer.sign,
    bbox=bounds
)

# Calculate NDVI
ndvi = (data.B08 - data.B04) / (data.B08 + data.B04)
ndvi = ndvi.compute() #calculate the result

# Find the time slice closest to 2021-07-24
target_date = pd.to_datetime("2021-07-24")
time_diffs = abs(data.time - target_date)
closest_time_index = time_diffs.argmin()

# Select the data for the closest time slice
ndvi_slice = ndvi.isel(time=closest_time_index)

# Save as Parquet
ndvi_df = ndvi_slice.to_dataframe(name='NDVI').reset_index()
gdf_ndvi = gpd.GeoDataFrame(ndvi_df, geometry=gpd.points_from_xy(ndvi_df.x, ndvi_df.y), crs="EPSG:2263")
gdf_ndvi.to_parquet('../output/sentinel2_ndvi.parquet')
```

## 04 Landsat Data

```python
# Supress Warnings
import warnings
warnings.filterwarnings('ignore')

# Import common GIS tools
import numpy as np
import xarray as xr
import matplotlib.pyplot as plt
import rioxarray as rio
import rasterio
from matplotlib.cm import jet,RdYlGn

# Import Planetary Computer tools
import stackstac
import pystac_client
import planetary_computer
from odc.stac import stac_load
import geopandas as gpd
import pandas as pd

# Define the bounding box for the entire data region using (Latitude, Longitude)
# This is the region of New York City that contains our temperature dataset
lower_left = (40.75, -74.01)
upper_right = (40.88, -73.86)

# Calculate the bounds for doing an archive data search
# bounds = (min_lon, min_lat, max_lon, max_lat)
bounds = (lower_left[1], lower_left[0], upper_right[1], upper_right[0])

# Define the time window
# We will use a period of 3 months to search for data
time_window = "2021-06-01/2021-09-01"

stac = pystac_client.Client.open("https://planetarycomputer.microsoft.com/api/stac/v1")

search = stac.search(
    bbox=bounds,
    datetime=time_window,
    collections=["landsat-c2-l2"],
    query={"eo:cloud_cover": {"lt": 50},"platform": {"in": ["landsat-8"]}},
)

items = list(search.get_items())
print('This is the number of scenes that touch our region:',len(items))

# Define the pixel resolution for the final product
# Define the scale according to our selected crs, so we will use degrees
resolution = 30  # meters per pixel
scale = resolution / 111320.0 # degrees per pixel for crs=4326

data1 = stac_load(
    items,
    bands=["red", "green", "blue", "nir08"],
    crs="EPSG:2263", # Changed
    resolution=resolution,  # Meters
    chunks={"x": 2048, "y": 2048},
    dtype="uint16",
    patch_url=planetary_computer.sign,
    bbox=bounds
)

data2 = stac_load(
    items,
    bands=["lwir11"],
    crs="EPSG:2263", # Changed
    resolution=resolution,  # Meters
    chunks={"x": 2048, "y": 2048},
    dtype="uint16",
    patch_url=planetary_computer.sign,
    bbox=bounds
)

# Scale Factors for the RGB and NIR bands
scale1 = 0.0000275
offset1 = -0.2
data1 = data1.astype(float) * scale1 + offset1

# Scale Factors for the Surface Temperature band
scale2 = 0.00341802
offset2 = 149.0
kelvin_celsius = 273.15 # convert from Kelvin to Celsius
data2 = data2.astype(float) * scale2 + offset2 - kelvin_celsius

# Find the time slice closest to 2021-07-24
target_date = pd.to_datetime("2021-07-24")
time_diffs = abs(data2.time - target_date)
closest_time_index = time_diffs.argmin()

# Select the data for the closest time slice
lst_slice = data2.isel(time=closest_time_index)

# Save LST as Parquet
lst_df = lst_slice.to_dataframe(name='LST').reset_index()
gdf_lst = gpd.GeoDataFrame(lst_df, geometry=gpd.points_from_xy(lst_df.x, lst_df.y), crs="EPSG:2263")
gdf_lst = gdf_lst[['LST', 'geometry']] # Keep only necessary columns.
gdf_lst.to_parquet('../output/landsat_lst.parquet')
```

